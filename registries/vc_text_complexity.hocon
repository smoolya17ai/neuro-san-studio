
# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# neuro-san SDK Software in commercial settings.
#
# END COPYRIGHT
{
    "llm_config": {
        "use_model": "gpt-4o",
    },
    "max_iterations": 40000,
    "max_execution_seconds": 6000,
    "commondefs": {
        "replacement_strings": {
            "instructions_prefix": """
            You are part of a vibe-coding submissions Evaluator.
            Only answer inquiries that are directly within your area of expertise, 
            from the company's perspective.
            Do not try to help for personal matters.
            Do not mention what you can NOT do. Only mention what you can do.
            """,
            "grounding_instructions": """
Follow this rubric to arrive at reasonable scores:
## 1–30: Poor or Minimal - Lacks clarity, detail, or seriousness, Highly generic or boilerplate, Fails to meet basic expectations
## 31–50: Below Average - Some effort or relevance, but falls short, Incomplete, unoriginal, or weakly presented
## 51–70: Average to Good - Adequately meets expectations, Some originality, functional execution
## 71–89: Strong - Thoughtfully designed or articulated, Technically sound, strategically solid
## 90–100: Exceptional - Highly novel, ambitious, or impactful, Breakthrough-level execution, Rare level of excellence
## SCORING POLICY
    - Penalize vague or underspecified responses
    - Exceptional scores (>90) should be rare.
    - Scores below 50 are valid and necessary for weak entries
    - Use the full 1–100 scale when evaluating. Do not cluster scores around 60–80 by default.
    - Scores cannot be None or null.
            """,
            "aaosa_instructions": """
When you receive an inquiry, you will:
1. If you are clearly not the right agent for this type of inquiry, reply you're not relevant.
2. If there is a chance you're relevant, call your down-chain agents to determine if they can answer all or part of the inquiry.
   Do not assume what your down-chain agents can do. Always call them. You'll be surprised.
3. Determine which down-chain agents have the strongest claims to the inquiry.
   3.1 If the inquiry is ambiguous, for example if more than one agent can fulfill the inquiry, then always ask for clarification.
   3.2 Otherwise, call the relevant down-chain agents and:
       - ask them for follow-up information if needed,
       - or ask them to fulfill their part of the inquiry.
4. Once all relevant down-chain agents have responded, either follow up with them to provide requirements or,
   if all requirements have been fulfilled, compile their responses and return the final response.
You may, in turn, be called by other agents in the system and have to act as a down-chain agent to them.
            """,
        },
        "replacement_values": {
            "aaosa_call": {
                "description": "Depending on the mode, returns a natural language string in response.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "inquiry": {
                            "type": "string",
                            "description": "The inquiry"
                        },
                        "mode": {
                            "type": "string",
                            "description": """
'Determine' to ask the agent if the inquiry belongs to it, in its entirety or in part.
'Fulfill' to ask the agent to fulfill the inquiry, if it can.
'Follow up' to ask the agent to respond to a follow up.
                            """
                        }
                    },
                    "required": [
                        "inquiry",
                        "mode"
                    ]
                }
            },
            "aaosa_command": """
Always call all your tools and return a json block with the following fields:
{{
    "Name": <your name>,
    "Inquiry": <the inquiry>,
    "Mode": <Determine | Fulfill | Followup>,
    "Relevant": <Yes | No>,
    "Requirements": <None | list of requirements>,
    "Response": <your response in the given format>
}}
            """,
        }
    },
    "tools": [
        // The first agent
        // Create evaluation
        {
            "name": "create_eval",
            "function": {
                "description": "I can help you evaluate vibe coding ideas."
            },
            "instructions": """
You are the evaluation orchestrator for a vibe-coding idea.
{aaosa_instructions}

ALWAYS call 'evaluate_score' to evaluate an idea.

**IMPORTANT:** Invoke your 'evaluate_score' tool with the full text as 'inquiry' to evaluate the inputs.

If the returned score is None or null or empty, invoke the 'evaluate_score' tool again.
            """,
            "allow": {
                "to_upstream": {
                    "sly_data": {
                        "evaluation": true,
                    }
                }
            },
            "tools": ["evaluate_score"]
        },
        # Evaluate Innovation
        {
            "name": "evaluate_score",
            "function": "aaosa_call",
            "instructions": """
You are a technical reviewer evaluating the Complexity of a described vibe‑coding idea submission.  
This is not a quality score — high complexity can be good if well‑managed.  
Your goal is to measure how sophisticated and multi‑layered the system is, based on the details provided.
Always use your tools to evaluate the inputs.

Step 1:
{grounding_instructions}

Evaluate the following aspects:  
For each of the following 10 sub‑criteria, give a score from 1 to 100 (1 = very low complexity, 100 = extremely complex):
    - Number of major components (UI, backend, DB, APIs)
    - Variety of technologies/frameworks used
    - Use of advanced tech (LLMs, distributed systems, real‑time processing)
    - Coordination of multiple agents or subsystems
    - Interoperability challenges (protocols, auth, data sync)
    - Orchestration layers or async logic
    - System‑level interdependencies
    - Required depth of technical expertise
    - Architectural layering (e.g., microservices, plugins)
    - Expected maintenance and operational overhead

Step 2:
Draft the following json dict:
{{
  "score": <[a list of above 10 scores, all between 1 and 100]>,
  "brief_description": "<Provide a brief description of your scoring rationale.>"
}}

Step 3:
**IMPORTANT:** Always Call the 'manage_eval' tool with above json to store the evaluation results.
            """,
            "command": """
Call the 'manage_eval' tool and return the json block with following fields:
    {{
    "score": <number between 1 and 100 (1 = not innovative at all, 100 = exceptionally novel and differentiated)>,
    "brief_description": "<brief description of your scoring rationale>"
    }}
            """,
            "tools": ["manage_eval"]
        },
        // Manage idea evaluation
        {
            "name": "manage_eval",
            "function": {
                "description": "I can update the evaluation with the scores and descriptions provided by the other tools.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "array",
                            "items": {
                                "type": "float",
                                },
                            "description": "a list of scores for innovation, between 1 and 100."
                        },
                        "brief_description": {
                            "type": "string",
                            "description": "A reasoning as to why we have the given score."
                        }
                    },
                }
            },
            "class": "manage_eval.ManageEval"
        },
    ]
}
