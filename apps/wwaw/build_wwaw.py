from urllib.parse import urljoin, urlparse
import time
import argparse
import tldextract
import random
import string
import requests
import os
import hashlib
import re
from bs4 import BeautifulSoup
from typing import List, Tuple, Optional
from hocon_constants import (HOCON_HEADER_REMAINDER, HOCON_HEADER_START, LEAF_NODE_AGENT_TEMPLATE,
                             REGULAR_AGENT_TEMPLATE, TOP_AGENT_TEMPLATE)


TOTAL_AGENTS = 500
MAX_CHILDREN = 10
MAX_NAME_LEN = 40  # Cannot be more than 55
PAGE_LEN_MAX = 5000
MIN_PAGE_LEN = 200

START_URL = "https://www.cognizant.com/us/en"
AGENT_NETWORK_NAME = f"autogenerated_agent_network_{TOTAL_AGENTS}"
OUTPUT_PATH = "../../registries/"  # Make sure the new hocon is added to the manifest
AGENT_INSTRUCTION_PREFACE = ("You represent the following content from a web page for the user and can answer questions"
                             " related to this content. "
                             "The web page you represent is part of a larger web site, represented by your tools. "
                             "Always use your tools as an extension of your content. Your content might not be up to "
                             "date or complete, but your tools may have the latest or complementary information, so "
                             "always check with your tools too. "
                             "Make sure you give the user all the information you have in all the content you and your "
                             "tools represent."
                             "Here is the specific content that you represent and the remainder is represented by your "
                             "tools: ")
agent_counter = 0


def enforce_max_fanout(agents: dict, max_children: int = 10) -> dict:
    """
    Ensures that no agent in the given agent hierarchy has more than `max_children` direct children.

    If an agent exceeds the allowed fan-out, intermediate agents (branches) are created to group
    subsets of its children. These intermediate agents are inserted into the hierarchy, and the
    original agent's down_chains are replaced with references to the new intermediate agents.

    Args:
        agents (dict): A dictionary representing the agent hierarchy. Each key is an agent name,
                       and each value is a dictionary with "instructions", "down_chains", and "top_agent".
        max_children (int): Maximum number of direct children allowed per agent.

    Returns:
        dict: A new dictionary with the same structure as `agents` but with fan-out constraints enforced.
    """
    new_agents = dict(agents)  # Shallow copy is safe here

    for parent, data in list(agents.items()):
        children = data["down_chains"]
        if len(children) <= max_children:
            continue

        # Split into chunks
        chunks = [children[i:i + max_children] for i in range(0, len(children), max_children)]
        intermediate_names = []

        for idx, chunk in enumerate(chunks):
            intermediate_name = f"{parent}_branch_{idx}"
            intermediate_name = re.sub(r"[^a-zA-Z0-9\-]", "", intermediate_name).lower()

            # If for any reason the name matches parent, modify it
            while intermediate_name == parent or intermediate_name in new_agents:
                idx += 1
                intermediate_name = f"{parent}_branch_{idx}"
                intermediate_name = re.sub(r"[^a-zA-Z0-9\-]", "", intermediate_name).lower()
            instructions = \
                (f"{AGENT_INSTRUCTION_PREFACE} You are an intermediate agent, grouping {len(chunk)}"
                 f" sub-agents of {parent}.")
            instructions = instructions.replace('"', '').replace("'", "")
            # Add the intermediate agent
            new_agents[intermediate_name] = {
                "instructions": instructions,
                "down_chains": chunk,
                "top_agent": "false"
            }

            # Just collect the names for now
            intermediate_names.append(intermediate_name)

        # Overwrite the parent's down_chains with the new intermediate branches
        new_agents[parent]["down_chains"] = intermediate_names
    return new_agents

def enforce_fanout_recursive(agents, max_children):
    """
    Recursively enforces the maximum fan-out constraint on a hierarchy of agents.

    Applies `enforce_max_fanout` repeatedly until all agents in the hierarchy have no more than
    `max_children` direct children. This is necessary when intermediate agents introduced by a
    previous enforcement may themselves exceed the limit.

    Args:
        agents (dict): A dictionary representing the agent hierarchy.
        max_children (int): Maximum number of allowed direct children per agent.

    Returns:
        dict: A modified agent hierarchy with fan-out constraints fully enforced.
    """
    while True:
        updated_agents = enforce_max_fanout(agents, max_children)
        if all(len(agent["down_chains"]) <= max_children for agent in updated_agents.values()):
            return updated_agents
        agents = updated_agents

def add_agent(agents, agent_name: str, instructions: str, down_chains: list, top_agent: str = "false"):
    """
    Adds a new agent to the agent hierarchy with the specified attributes.

    Validates that the agent name is unique and does not self-reference in its down_chains.
    Updates the global agent counter and prints progress indicators every 100 and 5000 agents.

    Args:
        agents (dict): The dictionary storing the agent hierarchy.
        agent_name (str): Unique identifier for the new agent.
        instructions (str): Instructional content assigned to the agent.
        down_chains (list): List of names of agents that this agent links to as sub-agents.
        top_agent (str): String flag ("true" or "false") indicating whether this is the top agent in the network.

    Returns:
        str: A string representation of the newly added agent's data.
    """
    global agent_counter
    assert agent_name not in agents, f"Duplicate agent: '{agent_name}' already exists."
    assert agent_name not in down_chains, f"Self-reference detected: '{agent_name}' cannot have itself as child."
    agents[agent_name] = {
        "instructions": instructions,
        "down_chains": down_chains,
        "top_agent": top_agent,
    }
    agent_counter += 1
    if agent_counter % 100 == 0:
        print(".", end="", flush=True)
    if agent_counter % 5000 == 0:
        print(f" {agent_counter}")
    return str(agents[agent_name])

def random_id(prefix="", length=6):
    """
    Generates a random alphanumeric identifier with an optional prefix.

    The identifier consists of lowercase letters and digits, with a default total
    length of 6 characters (excluding the prefix).

    Args:
        prefix (str): Optional string to prepend to the random ID.
        length (int): Length of the random portion of the ID (default is 6).

    Returns:
        str: A string representing the generated ID.
    """
    return prefix + ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))

def get_clean_agent_name(url, html, existing_names=None):
    """
    Generates a clean, URL-based agent name derived from the HTML page title or URL path.

    Ensures that the name is URL-safe, lowercase, ASCII-only, hyphenated, and unique within the
    `existing_names` set. The result is truncated if necessary to stay within the allowed length.

    Args:
        url (str): The URL of the web page.
        html (str): The raw HTML content of the web page.
        existing_names (set, optional): A set of agent names already used. Ensures the result is unique.

    Returns:
        str: A clean, unique agent name suitable for use as an identifier.
    """
    if existing_names is None:
        existing_names = set()
    soup = BeautifulSoup(html, 'html.parser')
    title = soup.title.string.strip() if soup.title and soup.title.string else ""

    if not title:
        parsed = urlparse(url)
        title = parsed.path.strip("/") or parsed.netloc

    base = re.sub(r"[^\w\s-]", " ", title).lower()
    base = re.sub(r"[\s_]+", "-", base).strip("-")

    # Truncate cleanly to max allowed length
    base = base.encode("ascii", errors="ignore").decode()
    parts = base.split("-")

    # If already short, return it as-is
    if len("-".join(parts)) <= MAX_NAME_LEN:
        base = "-".join(parts)
    else:
        # Greedy add from start and end until hitting max length
        front, back = [], []
        total_len = 2  # For "--"
        i, j = 0, len(parts) - 1
        turn = True  # Alternate between front and back

        while i <= j:
            candidate = parts[i] if turn else parts[j]
            if total_len + len(candidate) + 1 > MAX_NAME_LEN:
                break
            if turn:
                front.append(candidate)
                i += 1
            else:
                back.insert(0, candidate)
                j -= 1
            total_len += len(candidate) + 1
            turn = not turn

        base = "-".join(front + ["-"] + back)

    # Ensure uniqueness only if needed
    if base in existing_names:
        hash_suffix = hashlib.md5(url.encode()).hexdigest()[:6]
        base = (base[:MAX_NAME_LEN - 7].rstrip("-") + "-" + hash_suffix)

    assert len(base) <= MAX_NAME_LEN, f"Agent name too long: '{base}', len={len(base)}>{MAX_NAME_LEN}."

    return base

def clean_and_extract_text(html):
    """
    Cleans and extracts readable text content from HTML.

    Removes non-content elements (e.g., scripts, styles, images), strips unnecessary attributes,
    extracts visible text from paragraph-level tags, and sanitizes the text by removing URLs,
    special characters, and non-ASCII content.

    Args:
        html (str): Raw HTML content of a web page.

    Returns:
        str: Cleaned and normalized text extracted from the HTML.
    """
    soup = BeautifulSoup(html, 'html.parser')

    # Remove unwanted tags entirely
    for tag in soup(['script', 'style', 'noscript', 'img', 'source', 'picture', 'svg']):
        tag.decompose()

    # Remove image srcset and media nonsense in attributes
    for tag in soup.find_all(True):
        for attr in ["src", "srcset", "data-src", "data-srcset", "alt", "title"]:
            if attr in tag.attrs:
                del tag.attrs[attr]

    # Extract visible paragraph-level content
    paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'li'])
    raw_text = ' '.join(p.get_text(separator=' ', strip=True) for p in paragraphs)

    # Remove URLs or scene7 junk using regex
    clean_text = re.sub(r"https?://\S+", "", raw_text)
    clean_text = re.sub(r"@\(.*?\)", "", clean_text)

    # Normalize to ascii-only and strip quotes
    clean_text = clean_text.replace('"', '').replace("'", "")
    clean_text = clean_text.encode("ascii", errors="ignore").decode()

    return clean_text.strip()

def is_valid_link(link, base_domain):
    """
    Determines whether a given link is a valid internal HTTP/HTTPS URL within the specified base domain.

    Args:
        link (str): The URL to validate.
        base_domain (str): The base domain that the link must belong to (e.g., "example.com").

    Returns:
        bool: True if the link is a valid internal HTTP/HTTPS link for the domain, False otherwise.
    """
    parsed = urlparse(link)
    return parsed.scheme in ("http", "https") and base_domain in parsed.netloc

def crawl(start_url, max_agents):
    """
    Crawls a website starting from the given URL and constructs a hierarchy of content-based agents.

    Fetches HTML pages, extracts and cleans their textual content, and creates uniquely named agents
    representing each page. Links between agents are created based on internal navigation links.

    The crawl respects a maximum number of agents and skips pages with insufficient content. It also avoids
    revisiting URLs and ensures agent names are unique and well-formed. Pages that donâ€™t meet content length
    requirements are excluded from the final network.

    Args:
        start_url (str): The root URL to begin crawling from.
        max_agents (int): Maximum number of agents (pages) to generate.

    Returns:
        dict: A dictionary representing the agent hierarchy, where each key is an agent name and each value is
              a dictionary with "instructions", "down_chains", and "top_agent" fields.
    """
    agents = {}
    visited = set()
    to_visit: List[Tuple[str, Optional[str]]] = [(start_url, None)]
    count = 0
    domain_info = tldextract.extract(start_url)
    base_domain = f"{domain_info.domain}.{domain_info.suffix}"
    existing_names = set()

    while to_visit and count < max_agents:
        url, parent_name = to_visit.pop(0)
        if url in visited:
            continue
        try:
            resp = requests.get(url, timeout=10)
            if "text/html" not in resp.headers.get("Content-Type", ""):
                continue
            text = clean_and_extract_text(resp.text)
            if len(text) < MIN_PAGE_LEN:
                continue  # Skip very light pages

            name = get_clean_agent_name(url, resp.text, existing_names)
            existing_names.add(name)
            clean_text = (
                text[:PAGE_LEN_MAX].replace('"', '').replace("'", "").encode("ascii",
                                                                             errors="ignore").decode())
            instructions = f"{AGENT_INSTRUCTION_PREFACE}\n\n{clean_text}"
            instructions = instructions.replace('"""', '').replace('"', '')

            is_top = "true" if parent_name is None else "false"
            add_agent(agents, name, instructions, [], is_top)
            if parent_name and parent_name in agents and name != parent_name:
                agents[parent_name]["down_chains"].append(name)
            count += 1
            visited.add(url)

            # Collect more internal links from the same page
            soup = BeautifulSoup(resp.text, 'html.parser')
            for a in soup.find_all("a", href=True):
                full_link = urljoin(url, a['href'])

                if is_valid_link(full_link, base_domain) and full_link not in visited:
                    already_queued = any(full_link == queued_url for queued_url, _ in to_visit)
                    if not already_queued:
                        to_visit.append((full_link, name))
        except Exception as e:
            print(f"Skipping {url} due to error: {str(e)}")
            continue

        # Optional: politeness delay
        time.sleep(random.uniform(0.3, 0.8))

    linked = set()
    for agent in agents.values():
        linked.update(agent["down_chains"])

    print(f"Generated {count} agents with real content.")
    return agents

def get_agent_network_hocon(agents, agent_network_name):
    """
    Converts the agent hierarchy dictionary into a HOCON-formatted string.

    Ensures that one agent is marked as the top agent (if not already set),
    formats each agent entry according to its type (top, regular, or leaf),
    and constructs a valid HOCON representation of the entire network.

    Args:
        agents (dict): The dictionary containing all agents with their attributes ("instructions", "down_chains",
        "top_agent").
        agent_network_name (str): The name of the agent network, used as the root identifier in the HOCON output.

    Returns:
        str: A HOCON-formatted string representing the complete agent network.
    """
    # Assign top_agent to the one with the most down_chains
    if not any(agent["top_agent"] == "true" for agent in agents.values()):
        most_children = sorted(
            agents.items(), key=lambda x: len(x[1]["down_chains"]), reverse=True
        )
        if most_children:
            top_agent_name = most_children[0][0]
            agents[top_agent_name]["top_agent"] = "true"
            print(f"Assigned top_agent to: {top_agent_name}")

    agent_network_hocon = HOCON_HEADER_START + agent_network_name + HOCON_HEADER_REMAINDER
    for agent_name, agent in agents.items():
        unique_tools = []

        for down_chain in agent["down_chains"]:
            if down_chain == agent_name:
                print(f"Warning: Agent '{agent_name}' directly references itself in down_chains.")
                continue  # Skip it entirely
            elif down_chain not in unique_tools:
                unique_tools.append(f'"{down_chain}"')

        tools = ",".join(unique_tools)

        if agent["top_agent"] == "true":
            an_agent = TOP_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
                tools,
            )
        elif agent["down_chains"]:
            an_agent = REGULAR_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
                tools,
            )
        else:
            an_agent = LEAF_NODE_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
            )
        agent_network_hocon += an_agent

    agent_network_hocon += "]\n}\n"

    return agent_network_hocon

# === Main Script Execution ===
# This script generates a hierarchical agent network by crawling a website starting from a specified URL.
# It extracts page content, builds agents with instructional context, enforces maximum fan-out rules,
# and outputs a HOCON-formatted agent registry file for integration into a multi-agent system.
#
# Optional command-line arguments:
# --total_agents: Number of agents to generate (default: TOTAL_AGENTS)
# --start_url: Starting point for the crawl (default: START_URL)
# --agent_network_name: Name for the generated agent network (default: AGENT_NETWORK_NAME)
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a hierarchy of web agents.")
    parser.add_argument("--total_agents", type=int, required=False,
                        help=f"Total number of agents to generate (default {TOTAL_AGENTS})")
    parser.add_argument("--start_url", type=str, required=False,
                        help=f"Starting URL (default {START_URL})")
    parser.add_argument("--agent_network_name", type=str, required=False,
                        help=f"Agent network name (default: {AGENT_NETWORK_NAME})")

    args = parser.parse_args()

    # Use defaults if args are missing
    the_start_url = args.start_url or START_URL
    the_total_agents = args.total_agents or TOTAL_AGENTS
    the_agent_network_name = args.agent_network_name or AGENT_NETWORK_NAME
    the_agents = crawl(the_start_url, the_total_agents)
    the_agents = enforce_fanout_recursive(the_agents, max_children=MAX_CHILDREN)
    the_linked = set()
    for an_agnt in the_agents.values():
        the_linked.update(an_agnt["down_chains"])

    # 1) single, safe hub insertion
    the_unlinked = [
        a for a in the_agents
        if a not in the_linked and the_agents[a]["top_agent"] == "false"
    ]
    if the_unlinked:
        the_hub_name = "unlinked_hub"
        # only create once
        if the_hub_name not in the_agents:
            add_agent(
                the_agents,
                the_hub_name,
                "You connect otherwise unlinked agents.",
                the_unlinked,
                "false"
            )

        # pick the real top (excluding the hub itself)
        candidates = {
            name: data
            for name, data in the_agents.items()
            if name != the_hub_name
        }
        the_top = max(candidates.items(), key=lambda x: len(x[1]["down_chains"]))[0]
        # guard against self-reference and double-appending
        if the_hub_name != the_top and the_hub_name not in the_agents[the_top]["down_chains"]:
            the_agents[the_top]["down_chains"].append(the_hub_name)

    # 2) FINAL SANITY: strip any remaining self-references
    for name, data in the_agents.items():
        data["down_chains"] = [child for child in data["down_chains"] if child != name]

    # now generate HOCON
    hocon = get_agent_network_hocon(the_agents, the_agent_network_name)

    # Write the agent network file
    file_path = OUTPUT_PATH + the_agent_network_name + ".hocon"
    # Ensure the directory exists
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(hocon)
    print(f"\n agent count: {agent_counter}")
    print("\nDone!\n")